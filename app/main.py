import os
import logging
import joblib
from dotenv import load_dotenv
from tensorflow import keras
from fastapi import FastAPI, Header, Depends, HTTPException, status
from fastapi import BackgroundTasks
from typing import Annotated
from app.dashboard import router as dashboard_router

# --- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ---
# utilsëŠ” ì¸ì¦, í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ, ì•Œë¦¼ ë“± ë³´ì¡° ê¸°ëŠ¥ ë‹´ë‹¹
from app import utils
# schemasëŠ” ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë° ê·œê²© ì •ì˜ ë‹´ë‹¹
from app.schemas import PredictRequest, PredictResponse
# inferenceëŠ” ëª¨ë¸ ì¶”ë¡  ë¡œì§ ë‹´ë‹¹
from app.inference import predict_prob, post_process, VERSION

#########################################
# ë¡œê·¸ ìƒì„±
logger = logging.getLogger()

# ë¡œê·¸ì˜ ì¶œë ¥ ê¸°ì¤€ ì„¤ì •
logger.setLevel(logging.INFO)

# log ì¶œë ¥ í˜•ì‹
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
#########################################


## =================================================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì´ˆê¸° ì„¤ì •
## =================================================================
# .env íŒŒì¼ ë˜ëŠ” AWS í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°’ ë¡œë“œ
# .env íŒŒì¼ì„ ì½ì–´ ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¡œ ë¡œë“œ
API_KEY = os.getenv("API_KEY", "happy")
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "")
THRESHOLD = float(os.getenv("PREDICTION_THRESHOLD", 0.5)) # ì„ê³„ê°’ ë¡œë“œ

## =================================================================
## 2. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì „ì—­ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ ë‹¨ 1íšŒ)
## MLOps ì„±ëŠ¥ ìµœì í™”
## MLOps í™˜ê²½ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ë©°, APIì˜ ì‘ë‹µ ì†ë„(Latency)ë¥¼ ë³´ì¥í•©
## =================================================================
MODEL_PATH = "model/best_model.keras"         ## ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
SCALER_PATH = "artifacts/minmax_scaler.joblib"    ## ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ê²½ë¡œ

try:
    ## ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    MODEL = keras.models.load_model(MODEL_PATH, compile=False)
    SCALER = joblib.load(SCALER_PATH)
    logging.info(f"[INFO] ëª¨ë¸({MODEL_PATH}) ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    ## íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¡œë“œ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì„œë²„ ì‹œì‘ì„ ì¤‘ë‹¨í•˜ì—¬ ë°°í¬ ì‹¤íŒ¨ë¥¼ ëª…í™•íˆ
    logging.info(f"[ERROR] ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹¤íŒ¨! ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—ëŸ¬: {e}")
    ## ë°°í¬ í™˜ê²½ì—ì„œëŠ” íŒŒì¼ì´ ì—†ì„ ë•Œ ì„œë²„ê°€ ì‹œì‘ë˜ì§€ ì•Šë„ë¡ ì˜ˆì™¸ ë°œìƒ
    raise RuntimeError(f"Failed to load ML assets: {e}")

## =================================================================
# 3. FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ë° ì¸ì¦ ì˜ì¡´ì„±
## =================================================================
def get_api_key(x_api_key: Annotated[str | None, Header(alias="x-api-key")] = None):
    """API Keyë¥¼ ì¶”ì¶œí•˜ê³  ì¸ì¦ ë¡œì§ì„ utils.pyì— ìœ„ì„"""
    if not utils.authenticate_api_key(x_api_key, API_KEY):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, 
            detail="Unauthorized: Invalid API Key"
        )

# API ì¸ì¦ì„ ì „ì—­ ì˜ì¡´ì„±ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ì— ì ìš© (ì„ íƒ ì‚¬í•­)
app = FastAPI(
    title="Melting Tank Quality API",
    version=VERSION,
    # dependencies=[Depends(get_api_key)] # ëª¨ë“  API ìš”ì²­ì— ì¸ì¦ ì ìš©
)

## ëŒ€ì‹œë³´ë“œ ë¼ìš°í„° ë“±ë¡
app.include_router(dashboard_router)

## =================================================================
# 4. ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
## =================================================================
@app.get("/")
def root():
    """ìƒíƒœ í™•ì¸ ë° ë²„ì „ ì •ë³´ ì œê³µ (Health Check)"""
    return {"message": "Melting Tank Quality API is running", "version": VERSION}

@app.post("/predict", dependencies=[Depends(get_api_key)], response_model=PredictResponse)
def predict(req: PredictRequest, background: BackgroundTasks):
    """
    ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„°ë¡œ ë¶ˆëŸ‰ë¥ ì„ ì˜ˆì¸¡í•˜ê³ , ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    # 1. ì˜ˆì¸¡ ì‹¤í–‰: ì „ì—­ ë¡œë“œëœ MODELê³¼ SCALERë¥¼ inference í•¨ìˆ˜ì— ì „ë‹¬
    try:
        prob_ng = predict_prob(
            readings=[r.model_dump() for r in req.readings], 
            model=MODEL, 
            scaler=SCALER
        )
    except ValueError as e:
        # ë°ì´í„° ê¸¸ì´ ë¯¸ë‹¬ ë“± inference.pyì—ì„œ ë°œìƒí•œ ìœ íš¨ì„± ê²€ì‚¬ ì—ëŸ¬ ì²˜ë¦¬
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
    except Exception as e:
        # ê¸°íƒ€ ì˜ˆì¸¡ ì˜¤ë¥˜
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Prediction failed due to server error.")

    # 2. ì˜ˆì¸¡ í›„ì²˜ë¦¬ ë° ë¼ë²¨ ê²°ì •
    label, th = post_process(prob_ng, THRESHOLD)

    # 3. MLOps ì•Œë¦¼ ë¡œì§ (utils.py ì‚¬ìš©)
    if label == "NG":
        message = f"ğŸš¨ ë¶ˆëŸ‰ ê°ì§€ ê²½ê³ ! ì˜ˆì¸¡ í™•ë¥ : {prob_ng:.2f} (ì„ê³„ê°’: {th})"
        background.add_task(utils.send_alert_notification, message, SLACK_WEBHOOK_URL)
        
    # 4. ê²°ê³¼ ë¡œê¹… (ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
    # utils.log_prediction_result(req.readings, prob_ng, label, VERSION) # ë¹„ë™ê¸° ë¡œê¹… êµ¬í˜„ ì‹œ ì‚¬ìš©

    return PredictResponse(prob_ng=prob_ng, label=label, threshold=th, version=VERSION)

@app.get("/healthz")
def healthz():
    return {"status": "ok"}

@app.get("/readyz")
def readyz():
    try:
        _ = SCALER  # ë¡œë“œ ì—¬ë¶€ í™•ì¸
        _ = MODEL   # ë¡œë“œ ì—¬ë¶€ í™•ì¸
        return {"ready": True, "version": VERSION}
    except Exception:
        raise HTTPException(status_code=503, detail="Not ready")